# Usage:

* Download the whole repository and extract it to a folder.
* Run main.py script via console prompt. 
* If you place your hand accurately enough, the model will classify the gesture. You should put your hand to visible green area in the opened webcam window

> Note: model is developed by using american sign language dataset from [here](https://www.kaggle.com/datamunge/sign-language-mnist). Therefore, it is recommended for you to follow the following diagram while using the model:
![american sign language](https://www.researchgate.net/publication/328396430/figure/fig1/AS:683619848830976@1539999081795/The-26-letters-and-10-digits-of-American-Sign-Language-ASL.jpg)
