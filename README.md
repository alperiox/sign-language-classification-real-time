# Usage:

* Download the whole repository and extract it to a folder.
* Run main.py script via console prompt. 
* If you place your hand accurately enough, the model will classify the gesture. You should put your hand to visible green area in the opened webcam window

> Note: model is developed by using american sign language dataset from [here](https://www.kaggle.com/datamunge/sign-language-mnist). Therefore, it is recommended for you to follow the following diagram while using the model:
![american sign language](https://storage.googleapis.com/kagglesdsdata/datasets/3258/5337/american_sign_language.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210405%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210405T112535Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=aa6bbb8425cf000f2bcbcba5465215669178c9d501d49caf46a4ff55e031137627befc93430decf002dd5b29d38b0fffbf9c3b99a5dc7654509a654afaca0e5c0f20f3f5f954f2397ae98f0e5a6f1cc4e31915bc63b5f101a36c87aa99bb3fa7b2cad52a5ab46f0d0c5cdb0fb50636ac962d535d665d7a9a646e94a38da28ac2840cb2d56e12fa734df4aabd33838fee0f3b1c08c9cb3a0b143d4eb612cd12430b5f3b3a40616c860f9321b3784511424c164938b32332af0f0af9ec188257d0f1f8842ad6a6af45b8a43cf27eba94b7f3efafc5f38fb58ccd0c135625b1826bff8c839c2d14d034908ac13fb3679dff32827830436383af941d193ebb51f9f1)
